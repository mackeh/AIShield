use ndarray::Array2;
use ort::session::{builder::GraphOptimizationLevel, Session};
use ort::value::Tensor;
use std::sync::{Arc, Mutex};

pub struct OnnxClassifier {
    session: Arc<Mutex<Session>>,
}

impl OnnxClassifier {
    pub fn new(model_path: &str) -> Result<Self, Box<dyn std::error::Error>> {
        let session = Session::builder()?
            .with_optimization_level(GraphOptimizationLevel::Level1)?
            .with_intra_threads(1)?
            .commit_from_file(model_path)?;

        Ok(Self {
            session: Arc::new(Mutex::new(session)),
        })
    }

    fn extract_features(&self, code: &str) -> [f32; 3] {
        let lines: Vec<&str> = code.lines().collect();
        let total_lines = lines.len() as f32;

        if total_lines == 0.0 {
            return [0.0, 0.0, 0.0];
        }

        // Feature 1: Comment Density
        let comment_lines = lines
            .iter()
            .filter(|l| {
                l.trim().starts_with("//") || l.trim().starts_with("#") || l.trim().starts_with("*")
            })
            .count() as f32;
        let comment_density = comment_lines / total_lines;

        // Feature 2: Avg Line Length
        let total_chars: usize = lines.iter().map(|l| l.len()).sum();
        let avg_line_length = (total_chars as f32 / total_lines).min(200.0);

        // Feature 3: Keyword Score
        let lower = code.to_lowercase();
        let keywords = [
            "generated by",
            "openai",
            "chatgpt",
            "copilot",
            "anthropic",
            "claude",
            "here is the code",
            "sure,",
        ];
        let mut keyword_count: f32 = 0.0;
        for k in keywords {
            if lower.contains(k) {
                keyword_count += 1.0;
            }
        }
        let keyword_score = keyword_count.min(10.0);

        [comment_density, avg_line_length, keyword_score]
    }

    pub fn name(&self) -> &str {
        "OnnxRandomForest"
    }

    pub fn score(&self, code: &str) -> f32 {
        let features = self.extract_features(code);

        // Create input tensor: Shape [1, 3]
        let input = Array2::from_shape_vec((1, 3), features.to_vec()).unwrap();
        let input_tensor = match Tensor::from_array(input) {
            Ok(t) => t,
            Err(e) => {
                eprintln!("Tensor conversion error: {}", e);
                return 0.0;
            }
        };

        // Run inference
        let mut session = self.session.lock().unwrap();
        let outputs = match session.run(ort::inputs![input_tensor]) {
            Ok(o) => o,
            Err(e) => {
                eprintln!("Inference error: {}", e);
                return 0.0;
            }
        };

        // Extract output
        let label_tensor = match outputs.get("label") {
            Some(t) => t,
            None => return 0.0, // Should not happen
        };

        // try_extract_tensor returns (shape, data_slice)
        let (_shape, data) = match label_tensor.try_extract_tensor::<i64>() {
            Ok(t) => t,
            Err(_) => return 0.0,
        };

        if let Some(first) = data.first() {
            if *first == 1 {
                return 1.0; // Is AI
            }
        }

        0.0
    }
}
